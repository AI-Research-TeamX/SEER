# SEER: Self-Guided Function Calling in LLMs via Stepwise Experience Recall

This repository contains the official implementation of **SEER (Stepwise ExperiencE Recall)**, accepted at **EMNLP 2025**.
SEER is a self-guided framework that enhances multi-step tool-use in large language models (LLMs) through **stepwise retrieval of past successful trajectories** and **continual experience accumulation**.

* ğŸ“„ Paper: *Self-Guided Function Calling in Large Language Models via Stepwise Experience Recall* (EMNLP 2025)
* ğŸ¯ Benchmarks: ToolQA, Ï„-bench
* ğŸ§© Core Idea: Dynamically retrieve task-relevant exemplars from an online experience pool to guide LLM function calling.

<p align="center">
  <img src="assets/seer_overview.png" alt="SEER Framework Overview"/>
</p>

---

## ğŸ”‘ Key Features

* **Stepwise Experience Recall**: Fine-grained retrieval based on **trajectory similarity**, **toolchain coverage**, and **intent alignment**.
* **Continual Experience Accumulation**: Automatically adds successful trajectories to the experience pool for self-improvement.
* **Tool-Augmented LLMs**: Supports SQL, Python, math, text retrieval, and graph tools.
* **Extensive Evaluation**: Achieves +6.1% (easy) and +4.7% (hard) improvement on ToolQA, with strong gains on Ï„-bench.

---

## ğŸ“‚ Repository Structure

```
.
â”œâ”€â”€ data/                 # Datasets and question sets
â”‚   â”œâ”€â”€ dataset_generation/   # Notebooks for generating ToolQA variants
â”‚   â”‚   â”œâ”€â”€ easy_questions/   # Easy question generation
â”‚   â”‚   â””â”€â”€ hard_questions/   # Hard question generation
â”‚   â””â”€â”€ questions/            # JSONL question files (easy & hard)
â”‚
â”œâ”€â”€ plot/                 # Scripts & figures for analysis and ablations
â”‚   â”œâ”€â”€ ablation.py
â”‚   â”œâ”€â”€ improvement.py
â”‚   â””â”€â”€ *.pdf / *.html
â”‚
â”œâ”€â”€ seer/                 # Core implementation of SEER
â”‚   â”œâ”€â”€ llm_agents.py         # LLM agent interface
â”‚   â”œâ”€â”€ llm_evaluator.py      # LLM-as-a-judge evaluator
â”‚   â”œâ”€â”€ fewshots.py           # Few-shot example manager
â”‚   â”œâ”€â”€ tdqa*.py              # ToolQA experiment entry points
â”‚   â”œâ”€â”€ prompt_library/       # Prompt selection (Recall) & embeddings
â”‚   â”œâ”€â”€ tools/                # Tool wrappers (SQL, Python, Math, Text, Graph, Table)
â”‚   â”œâ”€â”€ utils.py              # Utility functions
â”‚   â””â”€â”€ configs.json          # Experiment configs
â”‚
â”œâ”€â”€ requirements_pip.txt   # Pip dependencies
â”œâ”€â”€ requirements_conda.yaml# Conda environment
â””â”€â”€ README.md
```

---

## âš™ï¸ Installation

We recommend using **conda**:

```bash
conda env create -f requirements_conda.yaml
conda activate seer
```

Alternatively, with pip:

```bash
pip install -r requirements_pip.txt
```

---

## ğŸš€ Usage

```bash
python seer/tdqa.py
```

---

## ğŸ“Š Data

* **ToolQA** datasets are under `data/questions/` (easy & hard).
* Each domain (e.g., `airbnb`, `coffee`, `scirex`) has corresponding `.jsonl` files.
* **Dataset generation notebooks** (`data/dataset_generation/`) provide scripts to regenerate question sets.

---

## ğŸ“ˆ Results

SEER achieves strong performance improvements:

* **ToolQA**: +6.1% (easy) and +4.7% (hard) vs. strongest baseline
* **Ï„-bench**: Qwen2.5-72B + SEER reaches **51.84%**, approaching GPT-4o (54.76%).

For detailed numbers, see the paper and `plot/` folder.

---

## ğŸ“š Citation

If you find this code useful, please cite our paper:

```bibtex
@article{cui2025self,
  title={Self-Guided Function Calling in Large Language Models via Stepwise Experience Recall},
  author={Cui, Sijia and He, Aiyao and Xu, Shuai and Zhang, Hongming and Wang, Yanna and Zhang, Qingyang and Wang, Yajing and Xu, Bo},
  journal={arXiv preprint arXiv:2508.15214},
  year={2025}
}
```
